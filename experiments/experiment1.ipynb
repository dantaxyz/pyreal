{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and code to load in the data and model is used by both conditions and included in analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "import lightgbm\n",
    "import time\n",
    "\n",
    "AWS_BASE_URL = 'https://pyreal-data.s3.amazonaws.com/'\n",
    "data_url = urljoin(AWS_BASE_URL, \"usability_study/california.csv\")\n",
    "data = pd.read_csv(data_url)\n",
    "\n",
    "data = data[data[\"median_house_value\"] < 500000]\n",
    "\n",
    "X_orig = data.drop(\"median_house_value\", axis=1)\n",
    "y = data[\"median_house_value\"]\n",
    "\n",
    "model = lightgbm.Booster(model_file='model.model')\n",
    "\n",
    "cities_url = urljoin(AWS_BASE_URL, \"usability_study/cal_cities_lat_long.csv\")\n",
    "cities = pd.read_csv(cities_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Control Condition: No use of Pyreal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   housing_median_age   total_rooms  total_bedrooms    population  \\\n",
      "0         6893.014235 -16313.670432    -5190.310484  33760.753826   \n",
      "\n",
      "    households  median_income  ocean_proximity          city  \n",
      "0 -5475.789824  151472.623759     21198.829465  36963.710606  \n",
      "Runtime:  0.4570004940032959\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder as SklearnOneHotEncoder\n",
    "import shap\n",
    "\n",
    "start = time.time()\n",
    "x_to_encode = X_orig[[\"ocean_proximity\"]]\n",
    "ohe = SklearnOneHotEncoder(sparse=False).fit(x_to_encode)\n",
    "encoded_columns = ohe.get_feature_names(x_to_encode.columns)\n",
    "index = x_to_encode.index\n",
    "ocean_encoded = ohe.transform(x_to_encode)\n",
    "ocean_encoded = pd.DataFrame(ocean_encoded, columns=encoded_columns, index=index)\n",
    "X_explain = pd.concat([X_orig.drop(\"ocean_proximity\", axis=\"columns\"), ocean_encoded], axis=1)\n",
    "X_interpret = X_orig.copy()\n",
    "for index, row in cities.iterrows():\n",
    "    lat = row[\"Latitude\"]\n",
    "    lon = row[\"Longitude\"]\n",
    "    X_interpret.loc[(X_interpret[\"latitude\"] > lat-0.1) & (X_interpret[\"latitude\"] < lat+0.1) & (X_interpret[\"longitude\"] > lon-0.1) & (X_interpret[\"longitude\"] < lon+0.1), \"city\"] = row[\"Name\"]\n",
    "X_interpret = X_interpret.drop(\"latitude\", axis=1)\n",
    "X_interpret = X_interpret.drop(\"longitude\", axis=1)\n",
    "columns = X_explain.columns\n",
    "explainer = shap.Explainer(model, X_explain)\n",
    "explanation = explainer(X_explain.iloc[0:1])\n",
    "explanation_df = pd.DataFrame(explanation.values, columns=columns)\n",
    "encoded_features = [item for item in encoded_columns if item.startswith(\"ocean_proximity_\")]\n",
    "summed_contribution = explanation_df[encoded_features].sum(axis=1)\n",
    "explanation_df = explanation_df.drop(encoded_features, axis=\"columns\")\n",
    "explanation_df[\"ocean_proximity\"] = summed_contribution\n",
    "explanation_df[\"city\"] = explanation_df[\"longitude\"] + explanation_df[\"latitude\"]\n",
    "explanation_df = explanation_df.drop(\"longitude\", axis=1)\n",
    "explanation_df = explanation_df.drop(\"latitude\", axis=1)\n",
    "shap_explanation = explanation_df\n",
    "print(shap_explanation)\n",
    "print(\"Runtime: \", time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Experimental Condition: Using Pyreal"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   housing_median_age   total_rooms  total_bedrooms    population  \\\n",
      "0         6893.014235 -16313.670432    -5190.310484  33760.753826   \n",
      "\n",
      "    households  median_income  ocean_proximity          city  \n",
      "0 -5475.789824  151472.623759     21198.829465  36963.710606  \n",
      "Runtime:  0.5860147476196289\n"
     ]
    }
   ],
   "source": [
    "from pyreal.transformers import Transformer, fit_transformers, OneHotEncoder\n",
    "from pyreal.explainers import LocalFeatureContribution\n",
    "from pyreal.types.explanations.dataframe import AdditiveFeatureContributionExplanation\n",
    "\n",
    "start = time.time()\n",
    "class CityConverter(Transformer): #****\n",
    "  def __init__(self): #****\n",
    "    self.cities = cities #****\n",
    "  def data_transform(self, x): #****\n",
    "    for index, row in self.cities.iterrows():\n",
    "      lat = row[\"Latitude\"]\n",
    "      lon = row[\"Longitude\"]\n",
    "      x.loc[(x[\"latitude\"] > lat-0.1) & (x[\"latitude\"] < lat+0.1) & (x[\"longitude\"] > lon-0.1) & (x[\"longitude\"] < lon+0.1), \"city\"] = row[\"Name\"]\n",
    "    x = x.drop(\"latitude\", axis=1)\n",
    "    x = x.drop(\"longitude\", axis=1)\n",
    "    return x #****\n",
    "  def inverse_transform_explanation_additive_contributions(self, explanation): #****\n",
    "    explanation = explanation.get()\n",
    "    explanation[\"city\"] = explanation[\"longitude\"] + explanation[\"latitude\"]\n",
    "    explanation = explanation.drop(\"longitude\", axis=1)\n",
    "    explanation = explanation.drop(\"latitude\", axis=1)\n",
    "    return AdditiveFeatureContributionExplanation(explanation)\n",
    "one_hot_encoder = OneHotEncoder(columns=[\"ocean_proximity\"])\n",
    "city_converter = CityConverter()\n",
    "e_transformers = [one_hot_encoder]\n",
    "i_transformers = [city_converter]\n",
    "fit_transformers(e_transformers, X_orig)\n",
    "fit_transformers(i_transformers, X_orig)\n",
    "local_explainer = LocalFeatureContribution(model, x_train_orig=X_orig, y_orig=y, e_algorithm=\"shap\", e_transformers=e_transformers, i_transformers=i_transformers, fit_on_init=True)\n",
    "pyreal_explanation = local_explainer.produce(X_orig.iloc[0:1])\n",
    "print(pyreal_explanation)\n",
    "print(\"Runtime: \", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in explanations:  \n"
     ]
    },
    {
     "data": {
      "text/plain": "Empty DataFrame\nColumns: []\nIndex: []",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Difference in explanations:  \")\n",
    "pyreal_explanation.compare(shap_explanation)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}