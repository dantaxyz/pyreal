{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import shap\n",
    "\n",
    "# PART 1: Define and fit the transformers that will get the data to the state ====================\n",
    "#         required to run the explanation algorithm\n",
    "X_explain = impute(X_orig) # impute() is a helper function that imputes missing values\n",
    "columns_to_encode = X_explain.select_dtypes(include=[\"object\"]).columns\n",
    "to_encode = X_explain[columns_to_encode]\n",
    "ohe = OneHotEncoder(sparse=False).fit(to_encode)\n",
    "encoded_columns = ohe.get_feature_names(to_encode.columns)\n",
    "index = to_encode.index\n",
    "encoded = ohe.transform(to_encode)\n",
    "encoded_df = pd.DataFrame(encoded, columns=encoded_columns, index=index)\n",
    "X_explain = pd.concat(\n",
    "  [X_explain.drop(columns_to_encode, axis=\"columns\"), encoded_df], axis=1)\n",
    "columns = X_explain.columns\n",
    "# ================================================================================================\n",
    "\n",
    "# PART 2: Load in and generate a human-readable explanation for a model ==========================\n",
    "model = pickle.load(open(\"../tutorials/ames_housing/model.pkl\", \"rb\"))\n",
    "\n",
    "# Generate a SHAP-type explanation\n",
    "explainer = shap.Explainer(model, X_explain)\n",
    "explanation = explainer(X_explain.iloc[0:1])\n",
    "explanation_df = pd.DataFrame(explanation.values, columns=columns)\n",
    "\n",
    "# Convert explanation to a more human-readable form\n",
    "for col in columns_to_encode:\n",
    "    encoded_features = [item for item in encoded_columns if item.startswith(col+'_')]\n",
    "    summed_contribution = explanation_df[encoded_features].sum(axis=1)\n",
    "    explanation_df = explanation_df.drop(encoded_features, axis=\"columns\")\n",
    "    explanation_df[col] = summed_contribution\n",
    "descriptions = {...} # dictionary of feature name to human-readable description\n",
    "explanation_df = explanation_df.rename(descriptions, axis='columns')\n",
    "# ================================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PART 1: Define and fit the transformers that will get the data to the state ====================\n",
    "#         required to run the explanation algorithm\n",
    "object_columns = X_orig.select_dtypes(include=[\"object\"]).columns\n",
    "transformers = [Imputer(), OneHotEncoder(object_columns)]\n",
    "fit_transformers(transformers, X_orig)\n",
    "\n",
    "# PART 2: Load in and generate a human-readable explanation for a model ==========================\n",
    "descriptions = {...} # dictionary of feature name to human-readable description\n",
    "lfc = LocalFeatureContribution(\n",
    "    model=\"../tutorials/ames_housing/model.pkl\",\n",
    "    x_train_orig=X_orig, y_orig=y,\n",
    "    e_algorithm=\"shap\",\n",
    "    transformers=transformers,\n",
    "    feature_descriptions=descriptions)\n",
    "lfc.fit()\n",
    "explanation = lfc.produce(X_orig.iloc[0:1])\n",
    "# ================================================================================================"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PART 1: Define and fit the transformers that will get the data to the state ====================\n",
    "#         required to run the explanation algorithm\n",
    "# One-hot encode the categorical \"ocean_proximity\" feature\n",
    "x_to_encode = X_orig[[\"ocean_proximity\"]]\n",
    "ohe = SklearnOneHotEncoder(sparse=False).fit(x_to_encode)\n",
    "encoded_columns = ohe.get_feature_names(x_to_encode.columns)\n",
    "index = x_to_encode.index\n",
    "ocean_encoded = ohe.transform(x_to_encode)\n",
    "ocean_encoded = pd.DataFrame(ocean_encoded, columns=encoded_columns, index=index)\n",
    "X_explain = pd.concat([X_orig.drop(\"ocean_proximity\", axis=\"columns\"), ocean_encoded], axis=1)\n",
    "\n",
    "# Convert the latitude and longitude features to the nearest city\n",
    "X_interpret = X_orig.copy()\n",
    "for index, row in cities.iterrows():\n",
    "    lat = row[\"Latitude\"]\n",
    "    lon = row[\"Longitude\"]\n",
    "    X_interpret.loc[(X_interpret[\"latitude\"] > lat-0.1)\n",
    "                    & (X_interpret[\"latitude\"] < lat+0.1)\n",
    "                    & (X_interpret[\"longitude\"] > lon-0.1)\n",
    "                    & (X_interpret[\"longitude\"] < lon+0.1), \"city\"] = row[\"Name\"]\n",
    "X_interpret = X_interpret.drop(\"latitude\", axis=1)\n",
    "X_interpret = X_interpret.drop(\"longitude\", axis=1)\n",
    "columns = X_explain.columns\n",
    "# =================================================================================================\n",
    "\n",
    "# PART 2: Load in and generate a human-readable explanation for a model ==========================\n",
    "explainer = shap.Explainer(model, X_explain)\n",
    "explanation = explainer(X_explain)\n",
    "explanation = np.mean(np.absolute(explanation.values), axis=0).reshape(1, -1)\n",
    "explanation_df = pd.DataFrame(explanation, columns=columns)\n",
    "\n",
    "encoded_features = [item for item in encoded_columns if item.startswith(\"ocean_proximity_\")]\n",
    "summed_contribution = explanation_df[encoded_features].sum(axis=1)\n",
    "explanation_df = explanation_df.drop(encoded_features, axis=\"columns\")\n",
    "explanation_df[\"ocean_proximity\"] = summed_contribution\n",
    "explanation_df[\"city\"] = explanation_df[\"longitude\"] + explanation_df[\"latitude\"]\n",
    "explanation_df = explanation_df.drop(\"longitude\", axis=1)\n",
    "explanation_df = explanation_df.drop(\"latitude\", axis=1)\n",
    "shap_explanation = explanation_df\n",
    "# ================================================================================================"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PART 1: Define and fit the transformers that will get the data to the state ====================\n",
    "#         required to run the explanation algorithm\n",
    "# Define a transformer that convert latitude/longitude values to city names\n",
    "class CityConverter(Transformer):\n",
    "  def __init__(self, **kwargs):\n",
    "    self.cities = cities # a dataframe of longitude/latitude values for cities\n",
    "    super().__init__(**kwargs)\n",
    "  def data_transform(self, x):\n",
    "    for index, row in self.cities.iterrows():\n",
    "      lat = row[\"Latitude\"]\n",
    "      lon = row[\"Longitude\"]\n",
    "      x.loc[(x[\"latitude\"] > lat-0.1)\n",
    "            & (x[\"latitude\"] < lat+0.1)\n",
    "            & (x[\"longitude\"] > lon-0.1)\n",
    "            & (x[\"longitude\"] < lon+0.1), \"city\"] = row[\"Name\"]\n",
    "    x = x.drop(\"latitude\", axis=1)\n",
    "    x = x.drop(\"longitude\", axis=1)\n",
    "    return x\n",
    "  def transform_explanation_additive_contributions(self, explanation): #****\n",
    "    explanation = explanation.get()\n",
    "    explanation[\"city\"] = explanation[\"longitude\"] + explanation[\"latitude\"]\n",
    "    explanation = explanation.drop(\"longitude\", axis=1)\n",
    "    explanation = explanation.drop(\"latitude\", axis=1)\n",
    "    return AdditiveFeatureContributionExplanation(explanation)\n",
    "\n",
    "# Intitialize and fit the transformers\n",
    "one_hot_encoder = OneHotEncoder(columns=[\"ocean_proximity\"])\n",
    "city_converter = CityConverter(model=False, interpret=True)\n",
    "transformers = [one_hot_encoder, city_converter]\n",
    "fit_transformers(transformers, X_orig)\n",
    "# ================================================================================================\n",
    "\n",
    "# PART 2: Load in and generate a human-readable explanation for a model ==========================\n",
    "global_explainer = GlobalFeatureImportance(\n",
    "    model, x_train_orig=X_orig, y_orig=y, e_algorithm=\"shap\", transformers=transformers)\n",
    "global_explainer.fit()\n",
    "pyreal_explanation = global_explainer.produce()\n",
    "# ================================================================================================"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PART 1: Define and fit the transformers that will get the data to the state ====================\n",
    "#         required to run the explanation algorithm\n",
    "# Define a transformer that converts yes/no labels to boolean features\n",
    "def boolean_encode(X):\n",
    "    x_transform = X.copy()\n",
    "    for col in [\"schoolsup\", \"famsup\", \"paid\", \"activities\", \"nursery\", \"internet\", \"romantic\", \"higher\"]:\n",
    "        x_transform[col] = x_transform[col].replace(('yes', 'no'), (1, 0))\n",
    "    x_transform[\"famsize\"] = x_transform[\"famsize\"].astype('category')\n",
    "    x_transform[\"famsize\"] = x_transform[\"famsize\"].cat.set_categories(['LE3', 'GT3'])\n",
    "    x_transform[\"famsize\"] = x_transform[\"famsize\"].cat.reorder_categories(['LE3', 'GT3'])\n",
    "    x_transform[\"famsize\"] = x_transform[\"famsize\"].cat.codes\n",
    "    return x_transform\n",
    "\n",
    "# Transform the data to the algorithm-ready state\n",
    "X_explain = boolean_encode(X_orig)\n",
    "columns_to_encode = [\"school\", \"sex\", \"address\", \"Pstatus\", \"reason\", \"guardian\", \"Mjob\", \"Fjob\"]\n",
    "to_encode = X_orig[columns_to_encode]\n",
    "ohe = SklearnOneHotEncoder(sparse=False).fit(to_encode)\n",
    "encoded_columns = ohe.get_feature_names(to_encode.columns)\n",
    "index = to_encode.index\n",
    "encoded = ohe.transform(to_encode)\n",
    "encoded = pd.DataFrame(encoded, columns=encoded_columns, index=index)\n",
    "X_explain = pd.concat([X_explain.drop(columns_to_encode, axis=\"columns\"), encoded], axis=1)\n",
    "standard_scaler = StandardScaler()\n",
    "columns = X_explain.columns\n",
    "index = X_explain.index\n",
    "X_explain = pd.DataFrame(standard_scaler.fit_transform(X_explain), columns=columns, index=index)\n",
    "\n",
    "# PART 2: Load in and generate a human-readable explanation for a model ==========================\n",
    "model = pickle.load(open(\"../tutorials/student/model.pkl\", \"rb\"))\n",
    "\n",
    "# Train a decision tree model that makes similar predictions to the real model\n",
    "tree_explainer = tree.DecisionTreeClassifier()\n",
    "tree_explainer.fit(X_explain, model.predict(X_explain))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PART 1: Define and fit the transformers that will get the data to the state ====================\n",
    "#         required to run the explanation algorithm\n",
    "# Define a transformer that converts yes/no labels to boolean features\n",
    "class BooleanEncoder(Transformer):\n",
    "    def __init__(self, cols, **kwargs):\n",
    "        self.cols = cols\n",
    "        super().__init__(**kwargs)\n",
    "    def data_transform(self, x):\n",
    "        x_transform = x.copy()\n",
    "        for col in self.cols:\n",
    "            x_transform[col] = x_transform[col].replace(('yes', 'no'), (1, 0))\n",
    "        x_transform[\"famsize\"] = x_transform[\"famsize\"].astype('category')\n",
    "        x_transform[\"famsize\"] = x_transform[\"famsize\"].cat.set_categories(['LE3', 'GT3'])\n",
    "        x_transform[\"famsize\"] = x_transform[\"famsize\"].cat.reorder_categories(['LE3', 'GT3'])\n",
    "        x_transform[\"famsize\"] = x_transform[\"famsize\"].cat.codes\n",
    "        return x_transform\n",
    "    def inverse_transform_explanation(self, explanation):\n",
    "        return explanation\n",
    "\n",
    "# Define and fit the transformers\n",
    "onehotencoder = OneHotEncoder([\"school\", \"sex\", \"address\", \"Pstatus\",\n",
    "                              \"reason\", \"guardian\", \"Mjob\", \"Fjob\"])\n",
    "boolean_encoder = BooleanEncoder(\n",
    "    [\"schoolsup\", \"famsup\", \"paid\", \"activities\", \"nursery\", \"internet\", \"romantic\", \"higher\"])\n",
    "standard_scaler = DataFrameWrapper(StandardScaler())\n",
    "transformers = [onehotencoder, boolean_encoder, standard_scaler]\n",
    "fit_transformers(transformers, X_orig)\n",
    "\n",
    "# PART 2: Load in and generate a human-readable explanation for a model ==========================\n",
    "# Generate the explanation - a decision tree model that makes similar predictions to the real model\n",
    "dte = DecisionTreeExplainer(model=\"../tutorials/student/model.pkl\",\n",
    "                            x_train_orig=X_orig, transformers=transformers,\n",
    "                            is_classifier=True, max_depth=4,\n",
    "                            feature_descriptions=feature_descriptions,\n",
    "                            fit_on_init=True)\n",
    "explanation_pyreal = dte.produce()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}