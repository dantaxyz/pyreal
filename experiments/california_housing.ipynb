{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and code to load in the data and model is used by both conditions and included in analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "import lightgbm\n",
    "import time\n",
    "from sklearn.preprocessing import OneHotEncoder as SklearnOneHotEncoder\n",
    "\n",
    "AWS_BASE_URL = 'https://pyreal-data.s3.amazonaws.com/'\n",
    "data_url = urljoin(AWS_BASE_URL, \"usability_study/california.csv\")\n",
    "data = pd.read_csv(data_url)\n",
    "\n",
    "data = data[data[\"median_house_value\"] < 500000]\n",
    "\n",
    "X_orig = data.drop(\"median_house_value\", axis=1)\n",
    "y = data[\"median_house_value\"]\n",
    "\n",
    "x_to_encode = X_orig[[\"ocean_proximity\"]]\n",
    "ohe = SklearnOneHotEncoder(sparse=False).fit(x_to_encode)\n",
    "encoded_columns = ohe.get_feature_names(x_to_encode.columns)\n",
    "index = x_to_encode.index\n",
    "ocean_encoded = ohe.transform(x_to_encode)\n",
    "ocean_encoded = pd.DataFrame(ocean_encoded, columns=encoded_columns, index=index)\n",
    "X_explain = pd.concat([X_orig.drop(\"ocean_proximity\", axis=\"columns\"), ocean_encoded], axis=1)\n",
    "\n",
    "model = lightgbm.LGBMRegressor().fit(X_explain, y)\n",
    "\n",
    "cities_url = urljoin(AWS_BASE_URL, \"usability_study/cal_cities_lat_long.csv\")\n",
    "cities = pd.read_csv(cities_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Control Condition: No use of Pyreal, local feature contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder as SklearnOneHotEncoder\n",
    "import shap\n",
    "\n",
    "start = time.time()\n",
    "x_to_encode = X_orig[[\"ocean_proximity\"]]\n",
    "ohe = SklearnOneHotEncoder(sparse=False).fit(x_to_encode)\n",
    "encoded_columns = ohe.get_feature_names(x_to_encode.columns)\n",
    "index = x_to_encode.index\n",
    "ocean_encoded = ohe.transform(x_to_encode)\n",
    "ocean_encoded = pd.DataFrame(ocean_encoded, columns=encoded_columns, index=index)\n",
    "X_explain = pd.concat([X_orig.drop(\"ocean_proximity\", axis=\"columns\"), ocean_encoded], axis=1)\n",
    "X_interpret = X_orig.copy()\n",
    "for index, row in cities.iterrows():\n",
    "    lat = row[\"Latitude\"]\n",
    "    lon = row[\"Longitude\"]\n",
    "    X_interpret.loc[(X_interpret[\"latitude\"] > lat-0.1) & (X_interpret[\"latitude\"] < lat+0.1) & (X_interpret[\"longitude\"] > lon-0.1) & (X_interpret[\"longitude\"] < lon+0.1), \"city\"] = row[\"Name\"]\n",
    "X_interpret = X_interpret.drop(\"latitude\", axis=1)\n",
    "X_interpret = X_interpret.drop(\"longitude\", axis=1)\n",
    "columns = X_explain.columns\n",
    "explainer = shap.Explainer(model, X_explain)\n",
    "explanation = explainer(X_explain.iloc[0:500])\n",
    "explanation_df = pd.DataFrame(explanation.values, columns=columns)\n",
    "encoded_features = [item for item in encoded_columns if item.startswith(\"ocean_proximity_\")]\n",
    "summed_contribution = explanation_df[encoded_features].sum(axis=1)\n",
    "explanation_df = explanation_df.drop(encoded_features, axis=\"columns\")\n",
    "explanation_df[\"ocean_proximity\"] = summed_contribution\n",
    "explanation_df[\"city\"] = explanation_df[\"longitude\"] + explanation_df[\"latitude\"]\n",
    "explanation_df = explanation_df.drop(\"longitude\", axis=1)\n",
    "explanation_df = explanation_df.drop(\"latitude\", axis=1)\n",
    "shap_explanation = explanation_df #***\n",
    "print(shap_explanation)\n",
    "print(\"Runtime: \", time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Experimental Condition: Using Pyreal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyreal.transformers import Transformer, fit_transformers, OneHotEncoder\n",
    "from pyreal.explainers import LocalFeatureContribution\n",
    "from pyreal.types.explanations.dataframe import AdditiveFeatureContributionExplanation\n",
    "\n",
    "start = time.time()\n",
    "class CityConverter(Transformer): #****\n",
    "  def __init__(self, **kwargs): #****\n",
    "    self.cities = cities #****\n",
    "    super().__init__(**kwargs) #****\n",
    "  def data_transform(self, x): #****\n",
    "    for index, row in self.cities.iterrows():\n",
    "      lat = row[\"Latitude\"]\n",
    "      lon = row[\"Longitude\"]\n",
    "      x.loc[(x[\"latitude\"] > lat-0.1) & (x[\"latitude\"] < lat+0.1) & (x[\"longitude\"] > lon-0.1) & (x[\"longitude\"] < lon+0.1), \"city\"] = row[\"Name\"]\n",
    "    x = x.drop(\"latitude\", axis=1)\n",
    "    x = x.drop(\"longitude\", axis=1)\n",
    "    return x #****\n",
    "  def transform_explanation_additive_contributions(self, explanation): #****\n",
    "    explanation = explanation.get()\n",
    "    explanation[\"city\"] = explanation[\"longitude\"] + explanation[\"latitude\"]\n",
    "    explanation = explanation.drop(\"longitude\", axis=1)\n",
    "    explanation = explanation.drop(\"latitude\", axis=1)\n",
    "    return AdditiveFeatureContributionExplanation(explanation)\n",
    "one_hot_encoder = OneHotEncoder(columns=[\"ocean_proximity\"])\n",
    "city_converter = CityConverter(model=False, interpret=True)\n",
    "transformers = [one_hot_encoder, city_converter]\n",
    "fit_transformers(transformers, X_orig)\n",
    "local_explainer = LocalFeatureContribution(model, x_train_orig=X_orig, y_orig=y, e_algorithm=\"shap\", transformers=transformers, fit_on_init=True)\n",
    "pyreal_explanation = local_explainer.produce(X_orig.iloc[0:500])\n",
    "print(pyreal_explanation)\n",
    "print(\"Runtime: \", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Difference in explanations:  \")\n",
    "pyreal_explanation[0].compare(shap_explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Global Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 19399/19648 [00:48<00:00]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   housing_median_age  total_rooms  total_bedrooms    population   households  \\\n",
      "0         5703.277746  8538.330061     7876.322111  14273.961072  2172.921944   \n",
      "\n",
      "   median_income  ocean_proximity          city  \n",
      "0   34165.340693     28072.309062  75501.488753  \n",
      "Runtime:  48.12432837486267\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder as SklearnOneHotEncoder\n",
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "start = time.time()\n",
    "x_to_encode = X_orig[[\"ocean_proximity\"]]\n",
    "ohe = SklearnOneHotEncoder(sparse=False).fit(x_to_encode)\n",
    "encoded_columns = ohe.get_feature_names(x_to_encode.columns)\n",
    "index = x_to_encode.index\n",
    "ocean_encoded = ohe.transform(x_to_encode)\n",
    "ocean_encoded = pd.DataFrame(ocean_encoded, columns=encoded_columns, index=index)\n",
    "X_explain = pd.concat([X_orig.drop(\"ocean_proximity\", axis=\"columns\"), ocean_encoded], axis=1)\n",
    "X_interpret = X_orig.copy()\n",
    "for index, row in cities.iterrows():\n",
    "    lat = row[\"Latitude\"]\n",
    "    lon = row[\"Longitude\"]\n",
    "    X_interpret.loc[(X_interpret[\"latitude\"] > lat-0.1) & (X_interpret[\"latitude\"] < lat+0.1) & (X_interpret[\"longitude\"] > lon-0.1) & (X_interpret[\"longitude\"] < lon+0.1), \"city\"] = row[\"Name\"]\n",
    "X_interpret = X_interpret.drop(\"latitude\", axis=1)\n",
    "X_interpret = X_interpret.drop(\"longitude\", axis=1)\n",
    "columns = X_explain.columns\n",
    "explainer = shap.Explainer(model, X_explain)\n",
    "explanation = explainer(X_explain)\n",
    "explanation = np.mean(np.absolute(explanation.values), axis=0).reshape(1, -1)\n",
    "explanation_df = pd.DataFrame(explanation, columns=columns)\n",
    "encoded_features = [item for item in encoded_columns if item.startswith(\"ocean_proximity_\")]\n",
    "summed_contribution = explanation_df[encoded_features].sum(axis=1)\n",
    "explanation_df = explanation_df.drop(encoded_features, axis=\"columns\")\n",
    "explanation_df[\"ocean_proximity\"] = summed_contribution\n",
    "explanation_df[\"city\"] = explanation_df[\"longitude\"] + explanation_df[\"latitude\"]\n",
    "explanation_df = explanation_df.drop(\"longitude\", axis=1)\n",
    "explanation_df = explanation_df.drop(\"latitude\", axis=1)\n",
    "shap_explanation = explanation_df #***\n",
    "print(shap_explanation)\n",
    "print(\"Runtime: \", time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimental Condition: Using Pyreal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|===================| 19338/19648 [00:47<00:00]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   housing_median_age  total_rooms  total_bedrooms    population   households  \\\n",
      "0         5703.277746  8538.330061     7876.322111  14273.961072  2172.921944   \n",
      "\n",
      "   median_income  ocean_proximity          city  \n",
      "0   34165.340693     28072.309062  75501.488753  \n",
      "Runtime:  48.10335326194763\n"
     ]
    }
   ],
   "source": [
    "from pyreal.transformers import Transformer, fit_transformers, OneHotEncoder\n",
    "from pyreal.explainers import LocalFeatureContribution, GlobalFeatureImportance\n",
    "from pyreal.types.explanations.dataframe import AdditiveFeatureContributionExplanation\n",
    "\n",
    "start = time.time()\n",
    "class CityConverter(Transformer): #****\n",
    "  def __init__(self, **kwargs): #****\n",
    "    self.cities = cities #****\n",
    "    super().__init__(**kwargs) #****\n",
    "  def data_transform(self, x): #****\n",
    "    for index, row in self.cities.iterrows():\n",
    "      lat = row[\"Latitude\"]\n",
    "      lon = row[\"Longitude\"]\n",
    "      x.loc[(x[\"latitude\"] > lat-0.1) & (x[\"latitude\"] < lat+0.1) & (x[\"longitude\"] > lon-0.1) & (x[\"longitude\"] < lon+0.1), \"city\"] = row[\"Name\"]\n",
    "    x = x.drop(\"latitude\", axis=1)\n",
    "    x = x.drop(\"longitude\", axis=1)\n",
    "    return x #****\n",
    "  def transform_explanation_additive_contributions(self, explanation): #****\n",
    "    explanation = explanation.get()\n",
    "    explanation[\"city\"] = explanation[\"longitude\"] + explanation[\"latitude\"]\n",
    "    explanation = explanation.drop(\"longitude\", axis=1)\n",
    "    explanation = explanation.drop(\"latitude\", axis=1)\n",
    "    return AdditiveFeatureContributionExplanation(explanation)\n",
    "one_hot_encoder = OneHotEncoder(columns=[\"ocean_proximity\"])\n",
    "city_converter = CityConverter(model=False, interpret=True)\n",
    "transformers = [one_hot_encoder, city_converter]\n",
    "fit_transformers(transformers, X_orig)\n",
    "global_explainer = GlobalFeatureImportance(model, x_train_orig=X_orig, y_orig=y, e_algorithm=\"shap\", transformers=transformers, fit_on_init=True)\n",
    "pyreal_explanation = global_explainer.produce()\n",
    "print(pyreal_explanation)\n",
    "print(\"Runtime: \", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime:  0.1740279197692871\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pickle\n",
    "\n",
    "start = time.time()\n",
    "numeric_imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "categorical_imputer = SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")\n",
    "numeric_cols = X_orig.dropna(axis=\"columns\", how=\"all\") \\\n",
    "            .select_dtypes(include=\"number\").columns\n",
    "categorical_cols = X_orig.dropna(axis=\"columns\", how=\"all\") \\\n",
    "    .select_dtypes(exclude=\"number\").columns\n",
    "numeric_imputer.fit(X_orig[numeric_cols])\n",
    "categorical_imputer.fit(X_orig[categorical_cols])\n",
    "new_numeric_cols = numeric_imputer.transform(X_orig[numeric_cols])\n",
    "new_categorical_cols = categorical_imputer.transform(X_orig[categorical_cols])\n",
    "X_explain = pd.concat([pd.DataFrame(new_numeric_cols, columns=numeric_cols, index=X_orig.index),\n",
    "                          pd.DataFrame(new_categorical_cols, columns=categorical_cols,\n",
    "                                       index=X_orig.index)], axis=1)\n",
    "x_to_encode = X_explain[[\"ocean_proximity\"]]\n",
    "ohe = SklearnOneHotEncoder(sparse=False).fit(x_to_encode)\n",
    "encoded_columns = ohe.get_feature_names(x_to_encode.columns)\n",
    "index = x_to_encode.index\n",
    "ocean_encoded = ohe.transform(x_to_encode)\n",
    "ocean_encoded = pd.DataFrame(ocean_encoded, columns=encoded_columns, index=index)\n",
    "X_explain = pd.concat([X_explain.drop(\"ocean_proximity\", axis=\"columns\"), ocean_encoded], axis=1)\n",
    "sklearn_explanation = tree.DecisionTreeRegressor()\n",
    "results = model.predict(X_explain)\n",
    "sklearn_explanation.fit(X_explain, model.predict(X_explain))\n",
    "print(\"Runtime: \", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_30384\\3979499450.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[0mtransformers\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mimputer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mone_hot_encoder\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[0mfit_transformers\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me_transformers\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mX_orig\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 9\u001B[1;33m \u001B[0mlocal_explainer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mDecisionTreeExplainer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx_train_orig\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mX_orig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_orig\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtransformers\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtransformers\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfit_on_init\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mis_classifier\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     10\u001B[0m \u001B[0mpyreal_explanation\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlocal_explainer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mproduce\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Runtime: \"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mstart\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\github\\pyreal\\pyreal\\explainers\\dte\\decision_tree_explainer.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, model, x_train_orig, e_algorithm, is_classifier, max_depth, **kwargs)\u001B[0m\n\u001B[0;32m    116\u001B[0m             \u001B[0me_algorithm\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mchoose_algorithm\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    117\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0me_algorithm\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"surrogate_tree\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 118\u001B[1;33m             self.base_decision_tree = SurrogateDecisionTree(model, x_train_orig,\n\u001B[0m\u001B[0;32m    119\u001B[0m                                                             is_classifier, max_depth, **kwargs)\n\u001B[0;32m    120\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbase_decision_tree\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\github\\pyreal\\pyreal\\explainers\\dte\\surrogate_decision_tree.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, model, x_train_orig, is_classifier, max_depth, **kwargs)\u001B[0m\n\u001B[0;32m     29\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_classifer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mis_classifier\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     30\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmax_depth\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmax_depth\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 31\u001B[1;33m         \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mSurrogateDecisionTree\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx_train_orig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     32\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     33\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\github\\pyreal\\pyreal\\explainers\\dte\\base.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, model, x_train_orig, interpretable_features, **kwargs)\u001B[0m\n\u001B[0;32m     24\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx_train_orig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minterpretable_features\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     25\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minterpretable_features\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minterpretable_features\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 26\u001B[1;33m         \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mDecisionTreeExplainerBase\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx_train_orig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     27\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m     \u001B[1;33m@\u001B[0m\u001B[0mabstractmethod\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\github\\pyreal\\pyreal\\explainers\\base.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, model, x_train_orig, y_orig, feature_descriptions, classes, class_descriptions, transformers, fit_on_init, return_original_explanation)\u001B[0m\n\u001B[0;32m    139\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    140\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mfit_on_init\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 141\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    142\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    143\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\github\\pyreal\\pyreal\\explainers\\dte\\surrogate_decision_tree.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     36\u001B[0m         \u001B[0mTODO\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mPerhaps\u001B[0m \u001B[0muse\u001B[0m \u001B[0msklearn\u001B[0m\u001B[0;31m'\u001B[0m\u001B[0ms\u001B[0m \u001B[0mGridSearchCV\u001B[0m \u001B[0mto\u001B[0m \u001B[0mfind\u001B[0m \u001B[0mthe\u001B[0m \u001B[1;34m\"best\"\u001B[0m \u001B[0mtree\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     37\u001B[0m         \"\"\"\n\u001B[1;32m---> 38\u001B[1;33m         \u001B[0ma_dataset\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform_to_x_algorithm\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mx_train_orig\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     39\u001B[0m         \u001B[0mm_dataset\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform_to_x_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mx_train_orig\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     40\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexplainer_input_size\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0ma_dataset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\github\\pyreal\\pyreal\\explainers\\base.py\u001B[0m in \u001B[0;36mtransform_to_x_algorithm\u001B[1;34m(self, x_orig)\u001B[0m\n\u001B[0;32m    173\u001B[0m         \"\"\"\n\u001B[0;32m    174\u001B[0m         \u001B[0ma_transformers\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_get_transformers\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransformers\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0malgorithm\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 175\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mrun_transformers\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ma_transformers\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx_orig\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    176\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    177\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mtransform_to_x_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx_orig\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\github\\pyreal\\pyreal\\transformers\\base.py\u001B[0m in \u001B[0;36mrun_transformers\u001B[1;34m(transformers, x)\u001B[0m\n\u001B[0;32m     58\u001B[0m         \u001B[0mtransformers\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mtransformers\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     59\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mtransform\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtransformers\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 60\u001B[1;33m         \u001B[0mx_transform\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtransform\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_transform\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     61\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mx_transform\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     62\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\github\\pyreal\\pyreal\\transformers\\base.py\u001B[0m in \u001B[0;36mtransform\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    161\u001B[0m                 \u001B[0mThe\u001B[0m \u001B[0mtransformed\u001B[0m \u001B[0mdataset\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    162\u001B[0m         \"\"\"\n\u001B[1;32m--> 163\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata_transform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    164\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    165\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mfit_transform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mfit_params\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\github\\pyreal\\pyreal\\transformers\\impute.py\u001B[0m in \u001B[0;36mdata_transform\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     64\u001B[0m             \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_frame\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mT\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     65\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 66\u001B[1;33m         \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcategorical_cols\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     67\u001B[0m             \u001B[0mnew_numeric_cols\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumeric_imputer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumeric_cols\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     68\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnew_numeric_cols\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcolumns\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumeric_cols\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindex\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "from pyreal.explainers import DecisionTreeExplainer\n",
    "from pyreal.transformers import MultiTypeImputer\n",
    "\n",
    "start = time.time()\n",
    "one_hot_encoder = OneHotEncoder(columns=[\"ocean_proximity\"])\n",
    "imputer = MultiTypeImputer()\n",
    "transformers = [imputer, one_hot_encoder]\n",
    "fit_transformers(transformers, X_orig)\n",
    "local_explainer = DecisionTreeExplainer(model, x_train_orig=X_orig, y_orig=y, transformers=transformers, fit_on_init=True, is_classifier=False)\n",
    "pyreal_explanation = local_explainer.produce()\n",
    "print(\"Runtime: \", time.time()-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}