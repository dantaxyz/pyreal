{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kr9Ut95fpCN5"
   },
   "source": [
    "# Using Custom Transformers in Pyreal\n",
    "\n",
    "In this tutorial, we will be using Pyreal to investigate the California Housing Dataset.\n",
    "\n",
    "In order to generate useful explanations, we will making a few custom transformers, with functionality specific to this use-case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "This dataset includes 9 predictor variables, and one target variable. Each row in the dataset refers to a block of houses in California. The target variable is the median house value in this block.\n",
    "\n",
    "**Run the cell below to load in the California Housing Dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jo606ew7GUGS",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>ocean_proximity</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8264</th>\n",
       "      <td>-118.18</td>\n",
       "      <td>33.77</td>\n",
       "      <td>37</td>\n",
       "      <td>2653</td>\n",
       "      <td>754.0</td>\n",
       "      <td>1087</td>\n",
       "      <td>698</td>\n",
       "      <td>2.3523</td>\n",
       "      <td>NEAR OCEAN</td>\n",
       "      <td>325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5473</th>\n",
       "      <td>-118.46</td>\n",
       "      <td>33.99</td>\n",
       "      <td>52</td>\n",
       "      <td>1158</td>\n",
       "      <td>253.0</td>\n",
       "      <td>528</td>\n",
       "      <td>253</td>\n",
       "      <td>3.5234</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "      <td>334700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14422</th>\n",
       "      <td>-117.23</td>\n",
       "      <td>32.80</td>\n",
       "      <td>27</td>\n",
       "      <td>1297</td>\n",
       "      <td>355.0</td>\n",
       "      <td>776</td>\n",
       "      <td>337</td>\n",
       "      <td>2.4643</td>\n",
       "      <td>NEAR OCEAN</td>\n",
       "      <td>244400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20376</th>\n",
       "      <td>-118.86</td>\n",
       "      <td>34.16</td>\n",
       "      <td>16</td>\n",
       "      <td>1509</td>\n",
       "      <td>216.0</td>\n",
       "      <td>578</td>\n",
       "      <td>235</td>\n",
       "      <td>10.2614</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "      <td>410800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6157</th>\n",
       "      <td>-117.96</td>\n",
       "      <td>34.08</td>\n",
       "      <td>39</td>\n",
       "      <td>1076</td>\n",
       "      <td>338.0</td>\n",
       "      <td>1242</td>\n",
       "      <td>332</td>\n",
       "      <td>2.2679</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "      <td>151800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15193</th>\n",
       "      <td>-117.04</td>\n",
       "      <td>33.03</td>\n",
       "      <td>16</td>\n",
       "      <td>2852</td>\n",
       "      <td>435.0</td>\n",
       "      <td>1083</td>\n",
       "      <td>448</td>\n",
       "      <td>6.3761</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "      <td>296200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>-119.93</td>\n",
       "      <td>38.72</td>\n",
       "      <td>15</td>\n",
       "      <td>2061</td>\n",
       "      <td>465.0</td>\n",
       "      <td>573</td>\n",
       "      <td>196</td>\n",
       "      <td>2.2417</td>\n",
       "      <td>INLAND</td>\n",
       "      <td>97900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11066</th>\n",
       "      <td>-117.89</td>\n",
       "      <td>33.78</td>\n",
       "      <td>16</td>\n",
       "      <td>6352</td>\n",
       "      <td>1747.0</td>\n",
       "      <td>5085</td>\n",
       "      <td>1649</td>\n",
       "      <td>2.8835</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "      <td>193800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3104</th>\n",
       "      <td>-117.67</td>\n",
       "      <td>35.63</td>\n",
       "      <td>32</td>\n",
       "      <td>3661</td>\n",
       "      <td>787.0</td>\n",
       "      <td>1613</td>\n",
       "      <td>706</td>\n",
       "      <td>3.0687</td>\n",
       "      <td>INLAND</td>\n",
       "      <td>63500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11658</th>\n",
       "      <td>-118.02</td>\n",
       "      <td>33.82</td>\n",
       "      <td>21</td>\n",
       "      <td>2052</td>\n",
       "      <td>456.0</td>\n",
       "      <td>1173</td>\n",
       "      <td>432</td>\n",
       "      <td>3.7885</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "      <td>204500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "8264     -118.18     33.77                  37         2653           754.0   \n",
       "5473     -118.46     33.99                  52         1158           253.0   \n",
       "14422    -117.23     32.80                  27         1297           355.0   \n",
       "20376    -118.86     34.16                  16         1509           216.0   \n",
       "6157     -117.96     34.08                  39         1076           338.0   \n",
       "15193    -117.04     33.03                  16         2852           435.0   \n",
       "1023     -119.93     38.72                  15         2061           465.0   \n",
       "11066    -117.89     33.78                  16         6352          1747.0   \n",
       "3104     -117.67     35.63                  32         3661           787.0   \n",
       "11658    -118.02     33.82                  21         2052           456.0   \n",
       "\n",
       "       population  households  median_income ocean_proximity  \\\n",
       "8264         1087         698         2.3523      NEAR OCEAN   \n",
       "5473          528         253         3.5234       <1H OCEAN   \n",
       "14422         776         337         2.4643      NEAR OCEAN   \n",
       "20376         578         235        10.2614       <1H OCEAN   \n",
       "6157         1242         332         2.2679       <1H OCEAN   \n",
       "15193        1083         448         6.3761       <1H OCEAN   \n",
       "1023          573         196         2.2417          INLAND   \n",
       "11066        5085        1649         2.8835       <1H OCEAN   \n",
       "3104         1613         706         3.0687          INLAND   \n",
       "11658        1173         432         3.7885       <1H OCEAN   \n",
       "\n",
       "       median_house_value  \n",
       "8264               325000  \n",
       "5473               334700  \n",
       "14422              244400  \n",
       "20376              410800  \n",
       "6157               151800  \n",
       "15193              296200  \n",
       "1023                97900  \n",
       "11066              193800  \n",
       "3104                63500  \n",
       "11658              204500  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "\n",
    "AWS_BASE_URL = 'https://pyreal-data.s3.amazonaws.com/'\n",
    "data_url = urljoin(AWS_BASE_URL, \"usability_study/california.csv\")\n",
    "data = pd.read_csv(data_url)\n",
    "\n",
    "city_url = urljoin(AWS_BASE_URL, \"usability_study/cal_cities_lat_long.csv\")\n",
    "cities = pd.read_csv(city_url)\n",
    "\n",
    "data = data[data[\"median_house_value\"] < 500000]\n",
    "\n",
    "data = data.sample(5000, random_state=100)  # we will work with a truncated dataset to avoid memory crashes\n",
    "\n",
    "X = data.drop(\"median_house_value\", axis=1)\n",
    "y = data[\"median_house_value\"]\n",
    "\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yge_u_wO-dpB"
   },
   "source": [
    "We will be working with a pretrained model to predict the median house values for each block.\n",
    "\n",
    "**Run the code below to load in the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7FIJC51KGu53"
   },
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "import requests\n",
    "\n",
    "model_url = urljoin(AWS_BASE_URL, \"usability_study/model.model\")\n",
    "r = requests.get(model_url, allow_redirects=True)\n",
    "open(urljoin('data', 'model.model'), 'wb').write(r.content)\n",
    "\n",
    "model = lightgbm.Booster(model_file=urljoin('data', 'model.model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfmXzJH_HgiP"
   },
   "source": [
    "## Custom Transformers Basics\n",
    "\n",
    "Pyreal generates ML explanations using Explainer objects, which take in data Transformers through their `transformer` parameter. These Transformers take in three flags in their initialization, two of which we will use in this tutorial:\n",
    "\n",
    "- Transformers with a `model=True` flag take data from the original feature space (as we loaded in above) to the feature space used by the model.\n",
    "- Transformers with an `interpret=True` flag take data from the original feature space to a feature space more readable or interpretable by humans.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: white; border: 2px solid; padding: 10px\">\n",
    "    <b><i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>&nbsp; Note</b><br>\n",
    "    <p style=\"color: black\">\n",
    "         For information about the third transformer flag (<code>algorithm</code>), please see the <a href=\"https://github.com/sibyl-dev/pyreal/blob/dev/tutorials/advanced_explanation_generation.ipynb\">advanced explanation generation tutorial.</a>\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "The Pyreal `transformer` module has some common transformers available for use, but some use-cases may require you to write your own transformer. This can be done by extending the base `Transformer` class.\n",
    "\n",
    "### Transformer Functions\n",
    "When defining a custom transformer, you will need to consider three types of functions:\n",
    "- `data_transform` (*required*): A single function that transforms the data from space A to B.\n",
    "- `inverse_transform_explanation_XXX` (*optional*): Functions that transform an explanation from space B to A. This type of function only needs to be considered if the transformer is used by the explanation algorithm and leads to the data being more obfuscated/less interpretable (ie, will have the `algorithm` flag set to True and the `interpret` flag set to False)\n",
    "- `transform_explanation_XXX` (*optional*): Functions that transform an explanation from space A to B. This type of function only needs to be considered if the transformer is used to make data and explanations more interpretable than the algorithm-ready state (ie, will have the `algorithm` flag set to False and the `interpret` flag set to True)\n",
    "\n",
    "The `transform_explantion` type functions are written per Explanation output type. For this tutorial, we will consider additive local feature contribution and additive global feature importance explanations. At the end of this tutorial, we will consider some special cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZsXVvpTYaLs7"
   },
   "source": [
    "### Custom Transformer Example 1: Per-Household Averager\n",
    "\n",
    "Let's take a look at one possible custom transformer we can add, which will average the values of certain features per household. We will follow these steps to write the function:\n",
    "\n",
    "1. Define the transformer `__init__()` method, using a `super()` call for the parent `Transformer` class. The function can take optional arguments to configure the transformer. We will take in a list of columns to average.\n",
    "2. Define the `data_transform()` function, which takes an input DataFrame `x` and returns `x` after undergoing the transformation. In this case, we simply divide the selected columns by the households feature.\n",
    "3. Consider which flags we expect to be used with this transformer. In this case, our transformation will be used for the explanation algorithm, but also makes the data more interpretable, so our flags are `interpret=True` and `algorithm=True`. Therefore, we do not need to define any explanation transform functions for this use case.\n",
    "\n",
    "**Run the cell below to define the PerHouseholdAverager**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Yg9PZ5HDHD_C"
   },
   "outputs": [],
   "source": [
    "from pyreal.transformers import OneHotEncoder, MultiTypeImputer, Transformer, fit_transformers, run_transformers\n",
    "\n",
    "class PerHouseholdAverager(Transformer):\n",
    "    def __init__(self, columns, **kwargs):\n",
    "        # columns: the columns to average. Must be list of strings (column names)\n",
    "        self.columns = columns\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def data_transform(self, x):\n",
    "        # Transform the data by adding a new column from total_[column] called\n",
    "        #   average_[column]. This feature represents the average value of\n",
    "        #   [column] per household.\n",
    "        for column in self.columns:\n",
    "            name = column.replace(\"total\", \"average\")\n",
    "            x[name] = x[column] / x[\"households\"]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Transformer Example 2: City Converter\n",
    "\n",
    "Now, let's take a look at another custom transformer, this one requiring an explanation transform. This transformer will be used to convert latitude/longitude values into city areas, based on the closest city to the given coordinates.\n",
    "\n",
    "Again, we will follow the following steps:\n",
    "\n",
    "1. Define the transformer `__init__()` method, using a `super()` call for the parent `Transformer` class. The function can take optional arguments to configure the transformer. In this case, we will not take in any parameters.\n",
    "2. Define the `data_transform()` function, which takes an input DataFrame `x` and returns `x` after undergoing the transformation. In this case, we convert long/lat values to nearby cities.\n",
    "3. Consider which flags we expect to be used with this transformer. In this case, our transformation will be used to make the data more interpretable, but will not be fed into the model, so our flags are `interpret=True` and `algorithm=model=False`. Therefore, the `transform_explanation_XXX` functions will be called, but not the `inverse_transform_explanation` functions.\n",
    "4. Consider the explanation output types you are interested in. For now, let's consider additive local feature contribution and additive global feature importance explanations. These are explanations that provide an importance weighting to each feature, and where these importances can be meaningfully added together to represent the importance of multiple features. We will need to define `transform_explanation_additive_feature_contribution` and `transform_explanation_additive_feature_importance` functions.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: white; border: 2px solid; padding: 10px\">\n",
    "    <b><i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>&nbsp; Note</b><br>\n",
    "    <p style=\"color: black\">\n",
    "         Note that explanation output types are heirarchial, and Pyreal will use the explanation transforms of higher-level types when the lower-level types are not available. For example, if <code>transform_explanation_additive_feature_contribution</code> is not defined, Pyreal will attempt to use <code>transform_explanation_feature_contribution</code>.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyreal.types.explanations.feature_based import AdditiveFeatureContributionExplanation, AdditiveFeatureImportanceExplanation\n",
    "\n",
    "class CityConverter(Transformer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.cities = cities\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def data_transform(self, x):\n",
    "        # Converts latitude/longitude coordinates to closest city name. Note that\n",
    "        #    we are using a very rough estimate here, assuming constant size.\n",
    "        flag = False\n",
    "        if isinstance(x, pd.Series):\n",
    "            x = x.to_frame().T\n",
    "            flag = True\n",
    "        for index, row in self.cities.iterrows():\n",
    "            lat = row[\"Latitude\"]\n",
    "            lon = row[\"Longitude\"]\n",
    "            x.loc[(x[\"latitude\"] > lat-0.1) & (x[\"latitude\"] < lat+0.1) & (x[\"longitude\"] > lon-0.1) & (x[\"longitude\"] < lon+0.1), \"city\"] = row[\"Name\"]\n",
    "        x = x.drop(\"latitude\", axis=1)\n",
    "        x = x.drop(\"longitude\", axis=1)\n",
    "        if flag:\n",
    "            x = x.squeeze(axis=0)\n",
    "        return x\n",
    "\n",
    "    def transform_explanation_additive_feature_contribution(self, explanation):\n",
    "        df = explanation.get()  # A DataFrame with one row per instance and one column per feature\n",
    "        return AdditiveFeatureContributionExplanation(CityConverter.helper_sum_lat_long_columns(df))\n",
    "\n",
    "    def transform_explanation_additive_feature_importance(self, explanation):\n",
    "        df = explanation.get()  # A DataFrame with one row and one column per feature\n",
    "        return AdditiveFeatureImportanceExplanation(CityConverter.helper_sum_lat_long_columns(df))\n",
    "\n",
    "    def helper_sum_lat_long_columns(explanation):\n",
    "        # In the case of additive contributions or importances, we can combine the latitude and\n",
    "        #    longitude explanation contributions by summing to get the city contribution. In this\n",
    "        #    case, our implementation is almost identical for both types of explanations\n",
    "        explanation[\"city\"] = explanation[\"longitude\"] + explanation[\"latitude\"]\n",
    "        explanation = explanation.drop(\"longitude\", axis=1)\n",
    "        explanation = explanation.drop(\"latitude\", axis=1)\n",
    "        return explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPtavPkfKf91"
   },
   "source": [
    "## Using Transformers\n",
    "\n",
    "In the next code cell, we will using our custom defined transformers, as well as some predefined Pyreal transformers, to generate local and global explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: training_size not provided. Defaulting to train with full dataset, running time might be slow.\n",
      "Warning: training_size not provided. Defaulting to train with full dataset, running time might be slow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: training_size not provided. Defaulting to train with full dataset, running time might be slow.\n",
      "Warning: training_size not provided. Defaulting to train with full dataset, running time might be slow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 14)\n",
      "(5000, 14)\n"
     ]
    }
   ],
   "source": [
    "from pyreal.explainers import LocalFeatureContribution, GlobalFeatureImportance\n",
    "from pyreal.transformers import OneHotEncoder, MultiTypeImputer, fit_transformers\n",
    "\n",
    "# Initialize and fit transformers using fit_transformers\n",
    "transformers = [MultiTypeImputer(model=True),\n",
    "                OneHotEncoder(columns=\"ocean_proximity\", model=True),\n",
    "                PerHouseholdAverager(columns=[\"total_bedrooms\", \"total_rooms\"], model=True, interpret=True),\n",
    "                CityConverter(model=False, interpret=True)]\n",
    "fit_transformers(transformers, X)\n",
    "\n",
    "# Initialize and fit the explainers\n",
    "local_explainer = LocalFeatureContribution(model=model, x_train_orig=X, transformers=transformers, fit_on_init=True)\n",
    "global_explainer = GlobalFeatureImportance(model=model, x_train_orig=X, transformers=transformers, fit_on_init=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "L5N5xe0dIuTt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyreal.types.explanations.feature_based.AdditiveFeatureContributionExplanation object at 0x000002115815E130>\n",
      "(5000, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|===================| 4894/5000 [00:17<00:00]        "
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 305823.966181, while the model output was 296255.093609. If this difference is acceptable you can set check_additivity=False to disable this check.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mException\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_25556\\148151440.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;31m# Generate and visualize the explanation\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mlocal_explanation\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx_interpret\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlocal_explainer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mproduce\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0miloc\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m \u001B[0mglobal_explanation\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mglobal_explainer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mproduce\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[0mvisualize\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mplot_top_contributors\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlocal_explanation\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalues\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mx_interpret\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mselect_by\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"absolute\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mshow\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\github\\pyreal\\pyreal\\explainers\\gfi\\base.py\u001B[0m in \u001B[0;36mproduce\u001B[1;34m(self, x_orig)\u001B[0m\n\u001B[0;32m     51\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mimportance\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     52\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mimportance\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 53\u001B[1;33m         \u001B[0mimportance\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_importance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     54\u001B[0m         \u001B[0mimportance\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform_explanation\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimportance\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     55\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minterpretable_features\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\github\\pyreal\\pyreal\\explainers\\gfi\\global_feature_importance.py\u001B[0m in \u001B[0;36mget_importance\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    139\u001B[0m                 \u001B[0mContribution\u001B[0m \u001B[0mof\u001B[0m \u001B[0meach\u001B[0m \u001B[0mfeature\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0meach\u001B[0m \u001B[0minstance\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    140\u001B[0m         \"\"\"\n\u001B[1;32m--> 141\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbase_global_feature_importance\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_importance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\Documents\\github\\pyreal\\pyreal\\explainers\\gfi\\shap_feature_importance.py\u001B[0m in \u001B[0;36mget_importance\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     68\u001B[0m         \u001B[0mx_model_np\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0masanyarray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_model\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     69\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_model_np\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 70\u001B[1;33m         \u001B[0mshap_values\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexplainer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshap_values\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_model_np\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     71\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     72\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mshap_values\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mndim\u001B[0m \u001B[1;33m<\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pyreal-BgWg7FQV-py3.8\\lib\\site-packages\\shap\\explainers\\_tree.py\u001B[0m in \u001B[0;36mshap_values\u001B[1;34m(self, X, y, tree_limit, approximate, check_additivity, from_call)\u001B[0m\n\u001B[0;32m    404\u001B[0m         \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_get_shap_output\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mphi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mflat_output\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    405\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mcheck_additivity\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel_output\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"raw\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 406\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0massert_additivity\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    407\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    408\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mout\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pyreal-BgWg7FQV-py3.8\\lib\\site-packages\\shap\\explainers\\_tree.py\u001B[0m in \u001B[0;36massert_additivity\u001B[1;34m(self, phi, model_output)\u001B[0m\n\u001B[0;32m    537\u001B[0m                 \u001B[0mcheck_sum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexpected_value\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mphi\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel_output\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    538\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 539\u001B[1;33m             \u001B[0mcheck_sum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexpected_value\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mphi\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel_output\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    540\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    541\u001B[0m     \u001B[1;33m@\u001B[0m\u001B[0mstaticmethod\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pyreal-BgWg7FQV-py3.8\\lib\\site-packages\\shap\\explainers\\_tree.py\u001B[0m in \u001B[0;36mcheck_sum\u001B[1;34m(sum_val, model_output)\u001B[0m\n\u001B[0;32m    531\u001B[0m                            \u001B[1;34m\" was %f, while the model output was %f. If this difference is acceptable\"\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    532\u001B[0m                            \u001B[1;34m\" you can set check_additivity=False to disable this check.\"\u001B[0m \u001B[1;33m%\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0msum_val\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mind\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel_output\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mind\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 533\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0mException\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0merr_msg\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    534\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    535\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mtype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mphi\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mis\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mException\u001B[0m: Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 305823.966181, while the model output was 296255.093609. If this difference is acceptable you can set check_additivity=False to disable this check."
     ]
    }
   ],
   "source": [
    "from pyreal.utils import visualize\n",
    "\n",
    "# Generate and visualize the explanation\n",
    "local_explanation, x_interpret = local_explainer.produce(X.iloc[0])\n",
    "global_explanation = global_explainer.produce()\n",
    "\n",
    "visualize.plot_top_contributors(local_explanation, values=x_interpret, select_by=\"absolute\", show=True)\n",
    "visualize.plot_top_contributors(global_explanation, select_by=\"absolute\", show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSth0yZ1A2vW"
   },
   "source": [
    "# Generating Global Explanations\n",
    "\n",
    "You can now start using Pyreal Explainers to investigate the model. Remember to take a look at the links at the top of this page as needed.\n",
    "\n",
    "We will begin by generating a *global* explanation, or an explanation of how the model makes predictions in general.\n",
    "\n",
    "⭐**In the next cell, initialize, fit, and call the produce function on a `GlobalFeatureImportance` Explainer.**\n",
    "\n",
    "⭐**Remember, you will need to begin by initalizing and fitting the required transformers. Revisit the cells above for details on the transformers you will need and their flags. You can use the `fit_transformers` function to quickly fit them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBwHIp18D50V"
   },
   "outputs": [],
   "source": [
    "from pyreal.explainers import GlobalFeatureImportance\n",
    "from pyreal.transformers import fit_transformers\n",
    "from pyreal.utils import visualize\n",
    "\n",
    "# Step one: Initialize and fit transformers using fit_transformers\n",
    "# ---- Your code here ----\n",
    "\n",
    "# Step two: Initialize and fit the explainer\n",
    "# ---- Your code here ----\n",
    "\n",
    "# Step three: Generate and visualize the explanation\n",
    "# ---- Your code here ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8gOOucyKt5A"
   },
   "source": [
    "⭐**Now, please press the \"next\" button on the Qualtrics survey tab and answer the next set of questions, referencing this tab as needed.** ⭐\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuMCOFODGoUa"
   },
   "source": [
    "# Generating Local Explanations\n",
    "\n",
    "⭐ **Now, please consider the `sample_block` listed below, which refers to a hypothetical block of houses that might exist in California. Run the code block.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bultpbt_Ktq_"
   },
   "outputs": [],
   "source": [
    "sample_block = pd.Series({\n",
    "  \"longitude\": -122.23,\n",
    "  \"latitude\": 37.88,\n",
    "  \"housing_median_age\": 16,\n",
    "  \"total_rooms\": 672,\n",
    "  \"total_bedrooms\": 230,\n",
    "  \"population\": 220,\n",
    "  \"households\": 52,\n",
    "  \"median_income\":  5.3252,\n",
    "  \"ocean_proximity\": \"NEAR BAY\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KB0VFOR7K63p"
   },
   "source": [
    "We will now generate a *local* explanation, or an explanation of why the model makes the prediction it does for the specific `sample_block` above.\n",
    "\n",
    "⭐**In the next cell, initialize, fit, and call the produce function on a `LocalFeatureContribution` Explainer. Generate an explanation for the prediction of the `sample_block` above.**\n",
    "\n",
    "⭐**You can likely reuse the already-fit transformers from the previous section. If you would like to use any more, remember to fit them before using.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_qyjModSK6UQ"
   },
   "outputs": [],
   "source": [
    "from pyreal.explainers import LocalFeatureContribution\n",
    "from pyreal.transformers import fit_transformers\n",
    "from pyreal.utils import visualize\n",
    "\n",
    "# Step one: Initialize and fit the explainer\n",
    "# ---- Your code here ----\n",
    "\n",
    "# Step two: Generate and visualize the explanation\n",
    "# ---- Your code here ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQ55r__NS8kQ"
   },
   "source": [
    "⭐**Now, please press the \"next\" button on the Qualtrics survey tab and answer the next set of questions, referencing this tab as needed.** ⭐\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbdTemKIf6lp"
   },
   "source": [
    "# Downloading This Notebook\n",
    "\n",
    "⭐**Once you have finished answering all questions, save and download this notebook to a location of your choosing using `File` &#8594; `Download` &#8594; `Download .ipynb`, in the upper left toolbar of this page. You will need to upload it shortly to the Qualtrics survey.**\n",
    "\n",
    "⭐**Please return to the Qualtrics survey now and press next, then follow instructions to upload this notebook, and then answer final reflection questions.**\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Pyreal Usability Study",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}