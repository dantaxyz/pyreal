{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16d578d4",
   "metadata": {},
   "source": [
    "## Simple Four-Feature-Spaces Tutorial\n",
    "\n",
    "In this tutorial, we will work through an simple, artificial ML problem to illustrate the full `Pyreal` workflow, including four unique feature spaces and multiple explanation types.\n",
    "\n",
    "Throughout this tutorial, imagine that we are Trinket Sellers looking to price new Trinkets based on five peices of information, as described below. We are working with ML model developers who will help us train a simple model to predict prices, but we would also like some explanations to help us understand how these predictions come from the five peices of information. As you will see, Pyreal can help!\n",
    "\n",
    "\n",
    "### Problem Setup\n",
    "\n",
    "We will be predicting Trinket prices based on five peices of information:\n",
    "1. Trinket Type `type` (categorical)\n",
    "2. Trinket Color `color` (categorical)\n",
    "3. Trinket Age `age` (integer)\n",
    "4. Trinket Weight `width` (float)\n",
    "4. Trinket Height `height` (float)\n",
    "\n",
    "This information can be formatted in multiple ways - each of these introduces a new *feature space*. In this tutorial, we will go over how you can use Pyreal Explainers and Transformers to play with these four feature spaces.\n",
    "\n",
    "### Original Feature Space\n",
    "\n",
    "In this example, we have been given the data in what we will refer to as the **original** feature space, or `X_orig`. This is an arbitrary feature space, that includes the information in whatever form our data-collection team happened to use. Let's load in and take a look at this space now.\n",
    "\n",
    "We see that we have seven columns, one each for items 1 through 4 above, plus three one-hot encoded columns for the `type` feature, which encodes three possible categories - `foo`, `bar`, and `foobar`. The color feature is listed using HTML color codes, which are not very understandable. The other three features are numeric, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5988b5d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'trinket_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30416\\1068088990.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata_orig\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"trinket_data.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0my_orig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_orig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"price\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX_orig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_orig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"price\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pyreal-BgWg7FQV-py3.8\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pyreal-BgWg7FQV-py3.8\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pyreal-BgWg7FQV-py3.8\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pyreal-BgWg7FQV-py3.8\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pyreal-BgWg7FQV-py3.8\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pyreal-BgWg7FQV-py3.8\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pyreal-BgWg7FQV-py3.8\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pyreal-BgWg7FQV-py3.8\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'trinket_data.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "filename = os.path.join(\"data\", \"trinket\", \"trinket_data.csv\")\n",
    "data_orig =  pd.read_csv(filename, index_col=0)\n",
    "y_orig = data_orig[\"price\"]\n",
    "X_orig = data_orig.drop(\"price\", axis=1)\n",
    "\n",
    "print(\"Original feature space:\")\n",
    "X_orig.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3442096c",
   "metadata": {},
   "source": [
    "### Model-Ready Feature Space\n",
    "\n",
    "Let's go ahead and train a model to predict the price variable from the features. To do so, we will first need to transform the data to a model-ready standpoint. Our ML development team has done their research and feature engineering, and suggested that a simple linear regression model may work well. They have also collected the following domain knowledge that may be useful for feature engineering:\n",
    "1. All trinkets are generally \"reddish\" or \"blueish\" - this is much more important to their price than the exact HTML color value.\n",
    "2. Bigger trinkets sell for more - but the difference between width and height is arbitrary. In fact, only the larger of these axis matter\n",
    "\n",
    "Based on this information, combined with the requirements of a linear regression model, let's put together some transformers using Pyreal's `transformer` package, and take a look at what the model-ready feature space looks like. As you can see, this feature space includes one-hot encoded and aggregated features.\n",
    "\n",
    "We will use some pre-defined Pyreal transformers, that cover common transformation types, and also define some of our own using the Transformer base class, such as converting HTML colors codes to `red` and `blue`.\n",
    "\n",
    "Note that Pyreal transformers take three flags: `model`, `interpret`, and `algorithm`. For now, we are just using the former. By setting `model` to `True`, we are telling Pyreal Explainers that these transformers are required to get the data to the feature space expected by the model. Note that `model==True` is the default flag; for the purposes of being clear in this tutorial, we are setting it explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2a5fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyreal.transformers import Transformer, OneHotEncoder, Mappings, MappingsOneHotDecoder, FeatureSelectTransformer\n",
    "from pyreal.transformers import fit_transformers, run_transformers\n",
    "\n",
    "\n",
    "def hex_to_color_name(h):\n",
    "    h = h.lstrip('#')\n",
    "    rgb = tuple(int(h[i:i+2], 16) for i in (0, 2, 4))\n",
    "    return [\"red\", \"green\", \"blue\"][rgb.index(max(rgb))]\n",
    "\n",
    "class ColorTransformer(Transformer):\n",
    "    \"\"\"\n",
    "    Transforms a hex color to `red` or `blue`\n",
    "    \"\"\"\n",
    "    def __init__(self, columns, **kwargs):\n",
    "        self.columns = columns\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def fit(self, x):\n",
    "        return self\n",
    "\n",
    "    def data_transform(self, x):\n",
    "        for col in self.columns:\n",
    "            x[col] = x[col].apply(hex_to_color_name)\n",
    "        return x\n",
    "\n",
    "class MaxAggregator(Transformer):\n",
    "    \"\"\"\n",
    "    Converts a set of numeric features to a single feature of the max value\n",
    "    \"\"\"\n",
    "    def __init__(self, columns, **kwargs):\n",
    "        self.columns = columns\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def data_transform(self, x):\n",
    "        column_name = \"MAX(\"\n",
    "        column_name += \",\".join(self.columns)\n",
    "        column_name += \")\"\n",
    "        x[column_name] = x[self.columns].max(axis=1)\n",
    "        x = x.drop(self.columns, axis=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "colorTransformer = ColorTransformer(columns = [\"color\"], model=True)\n",
    "colorEncoder = OneHotEncoder(columns = [\"color\"], model=True)\n",
    "maxAggregator = MaxAggregator(columns=[\"width\", \"height\"], model=True)\n",
    "# The featureSelect transformer keeps the order of columns consistent,\n",
    "#   which can be helpful with complex transformations\n",
    "featureSelect = FeatureSelectTransformer(['age', 'type_bar', 'type_foo', 'type_foobar',\n",
    "                                          'color_blue', 'color_green', 'color_red',\n",
    "                                          'MAX(width,height)'], model=True)\n",
    "\n",
    "model_transformers = [maxAggregator, colorTransformer, colorEncoder, featureSelect]\n",
    "X_model = fit_transformers(model_transformers, X_orig)\n",
    "print(\"Model ready feature space:\")\n",
    "X_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d39c1c",
   "metadata": {},
   "source": [
    "And now let's train a model and check the score..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144f4bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_model[0:400], y_orig[0:400])\n",
    "print(\"Model r-squared: %.4f\" % model.score(X_model[401:], y_orig[401:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6419468e",
   "metadata": {},
   "source": [
    "Our model development team has confirmed that this model performs well - but we would like to know how its making its predictions. How are our features interacting to give a price? Let's address this question by generating a local explanation for the first item in our dataset, using the popular explanation algorithm SHAP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f29df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from pyreal.utils import visualize\n",
    "\n",
    "explainer = shap.LinearExplainer(model, shap.maskers.Independent(data = X_model))\n",
    "shap_values = explainer.shap_values(X_model[0:1])[0]\n",
    "explanation = pd.DataFrame([shap_values], columns=X_model.columns)\n",
    "\n",
    "visualize.plot_top_contributors(explanation, select_by=\"absolute\",\n",
    "                                values=X_model.iloc[0], show=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320d7618",
   "metadata": {},
   "source": [
    "The explanation above can be hard to parse. The fact that the trinket is not of type `foobar` is greatly decreasing the predicted price; the fact that it is not of type `foo` is greatly increasing it. This level of granularity, including things like one-hot-encoded features, may be useful for some users - especially ML experts looking to work on the model itself. But what about other users, for example Trinket Sellers looking to use this model to help them decide on an appropriate Trinket price? Let's try some explanations presented in alternative ways."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aca8af1",
   "metadata": {},
   "source": [
    "### Interpretable feature space\n",
    "\n",
    "There are some transforms we can apply to help. Our users want to see the features that actually contribute to the model prediction, but are confused when features are presented one-hot-encoded. Let's compile a few more transformers (from the original feature space again) - including some that were also useful for getting to the model-ready feature space. We'll take a look at the interpretable feature space.\n",
    "\n",
    "We want to avoid one-hot encoded features, and present colors using English descriptions instead of HTML codes. Whether or not the explanation should include contributions from height and width separately versus the max aggregated feature is debatable - the former better matches the full information given, while the latter accurately describes how the model works. As we'll see in the next section, not all explanation types can accurately support all feature spaces, so that will factor into our choice as well.\n",
    "\n",
    "To use a transformer in the interpretable feature space, we set its `interpret` flag to `True`. We can update existing transformers' flags using the `set_flags()` method.\n",
    "\n",
    "We could make this space even more clear to human users with more descriptive feature names - but we'll add that in later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf146abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = Mappings.generate_mappings(\n",
    "    categorical_to_one_hot={\"type\": {\"type_foo\": \"foo\", \"type_bar\": \"bar\", \"type_foobar\": \"foobar\"}})\n",
    "typeDecoder = MappingsOneHotDecoder(mappings, model=False, interpret=True)\n",
    "\n",
    "colorTransformer.set_flags(interpret=True)\n",
    "maxAggregator.set_flags(interpret=True)\n",
    "interpretable_transformers = [typeDecoder, colorTransformer, maxAggregator]\n",
    "\n",
    "X_interpret = fit_transformers(interpretable_transformers, X_orig)\n",
    "print(\"Interpretable feature space:\")\n",
    "X_interpret.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b73f8a",
   "metadata": {},
   "source": [
    "### Algorithm-ready Feature Space\n",
    "\n",
    "Many existing explanation algorithm implementations can't just take data in the interpretable form to produce an interpretable explanation - they have their own requirements for their input feature space.\n",
    "\n",
    "Pyreal handles the details of transforming both data and explanations between feature spaces, using its `Explainer` classes, which take in a `transformer` list parameter, and through the Transformer flags. Let's see how that works, and once again generate a local SHAP contribution explanation.\n",
    "\n",
    "The first thing we need to do is identify what feature space is required by the explanation type we'd like to use. We can take a look at the documentation for `ShapFeatureContribution` explainers to see that, in this case, the explanation algorithm expects data in the model-ready feature space. We can use the transformers we defined above, where those that take the data to the model-ready feature space have the flag `model=True`, and those that take the data to the interpretable feature space have the flag `interpret=True`.\n",
    "\n",
    "We do need to be a bit careful with the order here. Pyreal calls the transformers' `.transform()` methods in the order listed, backtracking as needed to undo transformations on explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f640fb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyreal.explainers import ShapFeatureContribution\n",
    "\n",
    "transformers = [maxAggregator, colorTransformer, colorEncoder, featureSelect, typeDecoder]\n",
    "\n",
    "explainer = ShapFeatureContribution(model, X_orig,\n",
    "                                    transformers=transformers,\n",
    "                                    fit_on_init=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af087d60",
   "metadata": {},
   "source": [
    "We can now produce and visualize an explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec460e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation, X_interpret = explainer.produce(X_orig.iloc[0])\n",
    "print(explanation)\n",
    "print(X_interpret)\n",
    "visualize.plot_top_contributors(explanation, select_by=\"absolute\",\n",
    "                                values=X_interpret, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f10086",
   "metadata": {},
   "source": [
    "You may notice some warnings about missing transforms. It is not always possible to transform an explanation following a data transform - for example, in the case of MaxAggregator, we did not provide a way of distributing contribution between the features involved. Because of this, we will present our explanation using the aggregated features, to maintain explanation accuracy. We convert the data to the same interpretable state using the `i_transformers`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13948493",
   "metadata": {},
   "source": [
    "Pyreal makes it easy to switch between different explanation algorithms. We can get another feature-contribution-based explanation using the `SimpleCounterfactualContribution` explainer. This explainer requires features in a different feature space, using categorical features. To address this, we'll use the `algorithm` flag. When transformers have `algorithm=False`, the transformation will not be run until a model prediction is needed. The explanation algorithm itself will receive data transformed only using transformers with `algorithm=True`.\n",
    "\n",
    "In this case, the explanation generated has the height and width features un-aggregated, and so this is the most accurate output format. Because we decided either state could be interpretable, we will stick to the most accurate option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474530fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyreal.explainers import SimpleCounterfactualContribution\n",
    "\n",
    "typeEncoder = OneHotEncoder(columns = [\"type\"])\n",
    "fit_transformers([typeDecoder, typeEncoder], X_orig)\n",
    "\n",
    "typeDecoder.set_flags(model=True, algorithm=True, interpret=True)\n",
    "for transformer in [typeEncoder, maxAggregator, colorTransformer, colorEncoder, featureSelect]:\n",
    "    transformer.set_flags(model=True, algorithm=False, interpret=False)\n",
    "colorTransformer.set_flags(interpret=True)\n",
    "\n",
    "explainer = SimpleCounterfactualContribution(model, X_orig,\n",
    "                                             transformers=[typeDecoder, typeEncoder, maxAggregator, colorTransformer, colorEncoder, featureSelect],\n",
    "                                             fit_on_init=True)\n",
    "\n",
    "explanation, X_interpret = explainer.produce(X_orig.iloc[0:1])\n",
    "visualize.plot_top_contributors(explanation, select_by=\"absolute\",\n",
    "                                values=X_interpret.iloc[0], show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0013b104",
   "metadata": {},
   "source": [
    "If we did want to get contributions of the aggregated features, we could do so without sacrificing accuracy by aggregating at the explanation level rather than the model-ready level, which is supported by this explanation type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ee0b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxAggregator.set_flags(model=True, algorithm=True, interpret=True)\n",
    "explainer = SimpleCounterfactualContribution(model, X_orig,\n",
    "                                             transformers=[typeDecoder, maxAggregator, colorTransformer, colorEncoder, typeEncoder, featureSelect],\n",
    "                                             fit_on_init=True)\n",
    "\n",
    "explanation, X_interpret = explainer.produce(X_orig.iloc[0:1])\n",
    "visualize.plot_top_contributors(explanation, select_by=\"absolute\",\n",
    "                                values=X_interpret.iloc[0], show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2475cc3e",
   "metadata": {},
   "source": [
    "We can also generate global feature-based explanations using the `gfi` package - for example, the `ShapFeatureImportance` explainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1438a07c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyreal.explainers import ShapFeatureImportance\n",
    "\n",
    "for transformer in [colorTransformer, colorEncoder, featureSelect]:\n",
    "    transformer.set_flags(model=True, algorithm=True, interpret=False)\n",
    "typeDecoder.set_flags(model=False, algorithm=False, interpret=True)\n",
    "maxAggregator.set_flags(model=True, algorithm=True, interpret=True)\n",
    "\n",
    "explainer = ShapFeatureImportance(model, X_orig,\n",
    "                                  transformers=[maxAggregator, colorTransformer, colorEncoder, featureSelect, typeDecoder],\n",
    "                                  fit_on_init=True)\n",
    "\n",
    "explanation = explainer.produce()\n",
    "visualize.plot_top_contributors(explanation, select_by=\"absolute\", show=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
