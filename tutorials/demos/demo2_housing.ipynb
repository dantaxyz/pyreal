{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kr9Ut95fpCN5"
   },
   "source": [
    "# Developing Pyreal Applications\n",
    "\n",
    "In this tutorial, we will be going through the process of putting together a RealApp object for future use.\n",
    "\n",
    "If you have not already tried Demonstration 1: Using Pyreal, please do so before trying this demo, to better understand the purpose of RealApp objects.\n",
    "\n",
    "## Scenario Details\n",
    "### Goals for this Scenario\n",
    "1. Learn the components of a Pyreal RealApp, and the steps required to create one for your target application\n",
    "2. Learn about Pyreal's built-in Transformers, and how to make custom, application-specific transformers\n",
    "\n",
    "### User Details\n",
    "\n",
    "- **User Role:** ML Engineer for Real Estate Firm\n",
    "- **Expertise:**\n",
    "    - **Domain (House Pricing):** Some experience\n",
    "    - **Machine Learning:** Expert\n",
    "    - **Explainable ML techniques:** Basic understanding\n",
    "- **User Goals:**\n",
    "    1. Understand what features contribute to average block prices in California\n",
    "\n",
    "### Model and Data Details\n",
    "- 9 Features\n",
    "- Predicts average price of block of houses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "We will begin by loading in our data matrix and targets, and taking a look at the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyreal.sample_applications import california_housing\n",
    "import pandas as pd\n",
    "\n",
    "X, y = california_housing.load_data()\n",
    "X.sample(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Current State: Explaining Without Pyreal\n",
    "Let's start by fitting some transformers, training a model, and trying to generate a global explanation of our model without using Pyreal, so we have a point of comparison for both coding effort and explanation quality over just using ML and explanation algorithms directly.\n",
    "\n",
    "First, we transform the data and train a basic Perceptron..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_transform = X.copy()\n",
    "\n",
    "# One-hot encode ocean_proximity to be compatible with model\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "ohe_cols = ohe.fit_transform(X_transform[[\"ocean_proximity\"]])\n",
    "ohe_cols_df = pd.DataFrame(ohe_cols)\n",
    "column_names = ohe.get_feature_names_out([\"ocean_proximity\"])\n",
    "ohe_cols_df.columns = column_names\n",
    "X_transform = X_transform.drop(\"ocean_proximity\", axis=\"columns\")\n",
    "X_transform = pd.concat([X_transform, ohe_cols_df], axis=1)\n",
    "\n",
    "# Impute data to be compatible with model\n",
    "imp = SimpleImputer(strategy=\"mean\")\n",
    "imputed_bedrooms = imp.fit_transform(X_transform[[\"total_bedrooms\"]])\n",
    "X_transform[\"total_bedrooms\"] = imputed_bedrooms\n",
    "\n",
    "# Add additional columns\n",
    "X_transform[\"average_rooms\"] = X_transform[\"total_rooms\"] / X_transform[\"households\"]\n",
    "X_transform[\"average_bedrooms\"] = X_transform[\"total_bedrooms\"] / X_transform[\"households\"]\n",
    "\n",
    "columns = X_transform.columns\n",
    "\n",
    "# Scale data to improve model performance\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transform, y, random_state=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=columns) # We use DataFrame here to maintain feature names (column names)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=columns)\n",
    "\n",
    "expected_column_order = X_train.columns\n",
    "\n",
    "# Train model\n",
    "model = MLPRegressor(hidden_layer_sizes=(50, 50),\n",
    "                     max_iter=1000, solver=\"lbfgs\", alpha=0.001).fit(X_train, y_train)\n",
    "print(\"R^2 Score:\", model.score(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "... And then we can use SHAP to generate a beeswarm plot explanation of the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.Explainer(model.predict, X_train[0:500])\n",
    "shap_values = explainer(X_train[0:500])\n",
    "shap.plots.bar(shap_values[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, each point represents a row in our dataset. The farther right a point is, the more that specific feature value in that row *increased* the model prediction. The farther left a point is, the more that specific feature value in that row *decreased* the model prediction. The points are colored by feature value, with blue points representing a higher value.\n",
    "\n",
    "For example, we can see from this plot that higher latitudes and longitudes correspond to higher house prices. Similarly, blocks with fewer total rooms or higher populations are more expensive."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Developing Pyreal Applications: Model Transformers\n",
    "Often, data needs to be transformed before being passed into a machine learning model, either to improve performance or to format it to be supported by the model architecture. For example, in the code cells above we had to one-hot encode, impute, and standardize our data in order to make model predictions on it.\n",
    "\n",
    "Unfortunately, these kinds of transformation can make explanations hard to parse. For example, in the explanation above, we need to consider every `ocean_proximity` child feature together to get the total contribution of the actual block proximity to the ocean. Additionally, values are given are standardized values, which is difficult to parse.\n",
    "\n",
    "By replacing these transformers with the Pyreal built-in equivalents, our explanations will be automatically presented with these muddling transformations undone. Additionally, the `MultiTypeImputer` transformer automatically imputes all columns with an appropriate strategy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyreal.transformers import OneHotEncoder, MultiTypeImputer, DataFrameWrapper, FeatureSelectTransformer\n",
    "\n",
    "# Initialize and fit transformers using fit_transformers\n",
    "transformers = [MultiTypeImputer(),\n",
    "                OneHotEncoder(columns=\"ocean_proximity\")]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "While Pyreal offers many common transformer types, sometimes our specific domains may require a more specific transformer option. Luckily, Pyreal makes it easy to add new *custom transformers*. Let's start with a basic one.\n",
    "\n",
    "Below, we define the `PerHouseHoldAverager`. Pyreal Transformers extend the base `Transformer` class (line 3). They then define, at minimum, a `data_transform` function. This function acts just like the `.transform()` function of the sklearn transformers we used above - it takes in a dataframe, and returns the dataframe with one or more features transformed. In this case, our transformer adds a new column, with the per-household average of the requested features."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyreal.transformers import Transformer\n",
    "\n",
    "class PerHouseholdAverager(Transformer):\n",
    "    def __init__(self, columns, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            columns (list of strings)\n",
    "                The names of the columns to average\n",
    "        \"\"\"\n",
    "        self.columns = columns\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def data_transform(self, x):\n",
    "        \"\"\"\n",
    "        Transform the data by adding a new column from total_[column] called average_[column].\n",
    "        This feature represents the average value of [column] per household.\n",
    "\n",
    "        Args:\n",
    "            x (DataFrame)\n",
    "                The data to transform\n",
    "        \"\"\"\n",
    "        for column in self.columns:\n",
    "            name = column.replace(\"total\", \"average\")\n",
    "            x[name] = x[column] / x[\"households\"]\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's go ahead and add that transformer to our list. You may notice the `interpret` flag in our initialization. This flag tells Pyreal that this transformation improves the *interpretability* of the resulting data - or in other words, changes the presentation of the data in such a way that makes it easier to reason about (or at least, not more difficult).\n",
    "\n",
    "In this case, we are adding features (`average_rooms` and `average_bedrooms`) that we expect will be very easy for our users to reason about, so we do not need to undo this transformation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transformers.append(PerHouseholdAverager(columns=[\"total_bedrooms\", \"total_rooms\"], interpret=True))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sometimes, there may be a way to make the data even more interpretable (ie, easy to reason about) than the original data, but that for whatever reason we do not want to or cannot make this change to the data being fed into the model.\n",
    "\n",
    "For example, the latitude and longitude features above are hard to reason about for most people. Something like a city or neighborhood name would be better, but trying to one-hot encode every neighborhood in California to feed into the model would be a massive memory drain. Instead, we will continue giving the model the `longitude`/`latitude` features, but we will then transform the *explanation* to use this more interpretable feature.\n",
    "\n",
    "To do so, we will use another transformer. Take a look at the definition below; we will then walk through it step-by-step"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7FIJC51KGu53",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from pyreal.types.explanations.feature_based import AdditiveFeatureImportanceExplanation, AdditiveFeatureContributionExplanation\n",
    "\n",
    "def helper_sum_lat_long_columns(explanation):\n",
    "    \"\"\"\n",
    "    In the case of additive contributions or importances, we can combine the latitude and\n",
    "    longitude explanation contributions by summing to get the city contribution. In this\n",
    "    case, our implementation is almost identical for both types of explanations\n",
    "    \"\"\"\n",
    "    explanation_transform = explanation.copy()\n",
    "    explanation_transform[\"neighborhood\"] = explanation[\"longitude\"] + explanation[\"latitude\"]\n",
    "    explanation_transform = explanation_transform.drop(\"longitude\", axis=1)\n",
    "    explanation_transform = explanation_transform.drop(\"latitude\", axis=1)\n",
    "    return explanation_transform\n",
    "\n",
    "class CityConverter(Transformer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.cities = california_housing.load_city_data()\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def data_transform(self, x):\n",
    "        \"\"\"\n",
    "        Converts latitude/longitude coordinates to closest city name. Note that\n",
    "        we are using a very rough estimate here, for illustrative purposes, rather than\n",
    "        a more complicated but accurate nearest neighbor search\n",
    "        Args:\n",
    "            x (DataFrame)\n",
    "                The data to transform\n",
    "        \"\"\"\n",
    "        for index, row in self.cities.iterrows():\n",
    "            lat = row[\"Latitude\"]\n",
    "            lon = row[\"Longitude\"]\n",
    "            x.loc[(x[\"latitude\"] > lat-0.1) & (x[\"latitude\"] < lat+0.1) & (x[\"longitude\"] > lon-0.1) & (x[\"longitude\"] < lon+0.1), \"neighborhood\"] = row[\"Name\"]\n",
    "        x = x.drop(\"latitude\", axis=1)\n",
    "        x = x.drop(\"longitude\", axis=1)\n",
    "        return x\n",
    "\n",
    "    def transform_explanation_additive_feature_contribution(self, explanation):\n",
    "        \"\"\"\n",
    "        Combines the contributions of the latitude and longitude features for\n",
    "        additive local feature contribution explanations.\n",
    "        Args:\n",
    "            explanation (AdditiveFeatureImportanceExplanation)\n",
    "                The explanation to transform\n",
    "        \"\"\"\n",
    "        df = explanation.get()  # A DataFrame with one row per instance and one column per feature\n",
    "        return AdditiveFeatureContributionExplanation(helper_sum_lat_long_columns(df))\n",
    "\n",
    "    def transform_explanation_additive_feature_importance(self, explanation):\n",
    "        \"\"\"\n",
    "        Combines the importance of the latitude and longitude features for\n",
    "        additive local feature importance explanations.\n",
    "        Args:\n",
    "            explanation (AdditiveFeatureImportanceExplanation)\n",
    "                The explanation to transform\n",
    "        \"\"\"\n",
    "        df = explanation.get()  # A DataFrame with one row and one column per feature\n",
    "        return AdditiveFeatureImportanceExplanation(helper_sum_lat_long_columns(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "You are already familiar with the first two class methods in the `CityConverter` above:\n",
    "\n",
    "- [`init()`]: When initializing the transformer, we will load in a DataFrame connecting lat/long values with neighborhood names.\n",
    "- [`data_tranform()`]: The `data_transform` function converts the `latitude` and `longitude` columns in data into a `neighborhood` column using a simple algorithm.\n",
    "\n",
    "The next two functions are used to transform explanations. Explanation transform functions take in explanations objects of a specific type or category of types, and return the explanation transformed based on this transformer. We can write one explanation transform function per explanation output type:\n",
    "\n",
    "- [`transform_explanation_additive_feature_contribution`]: This function combines the contributions of the latitude and longitude features to get the contribution of the neighborhood feature. Because we only apply this transform to *additive* contributions, we can assume the contribution of a combination feature to be the sum of the original features.\n",
    "- [`transform_explanation_additive_feature_importance`]: This function is very similar to the previous, but acts on additive global importance explanations instead of local contributions.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: white; border: 2px solid; padding: 10px\">\n",
    "    <b><i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>&nbsp; Note</b><br>\n",
    "    <p style=\"color: black\">\n",
    "         Note that explanation output types are heirarchial, and Pyreal will use the explanation transforms of higher-level types when the lower-level types are not available. For example, if <code>transform_explanation_additive_feature_contribution</code> is not defined, Pyreal will attempt to use <code>transform_explanation_feature_contribution</code>.\n",
    "    </p>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transformers.append(CityConverter(interpret=True, model=False))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once again, we use the `interpret` flag to indicate that this transformation makes our data more interpretable. We also add the `model=False` flag, to indicate that we should **not** apply this model when making model predictions.\n",
    "\n",
    "We will also add two additional Pyreal built-in transformers, to improve model performance. Pyreal transformers are run in the order they appear in the transformer list, so we need to be careful about our order. The `DataFrameWrapper` wraps a standard scaler, allowing us to standardize all features while maintaining the DataFrame data type and column names. Finally, the `FeatureSelectTransformer` will reorder columns to ensure they are in the order expected by the model.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Custom Transformers: Summary\n",
    "\n",
    "In summary, when defining custom Pyreal transformers, we take the following steps:\n",
    "1. Define the transformer `__init__()` method, using a `super()` call for the parent `Transformer` class. The function can take optional arguments to configure the transformer.\n",
    "2. Define the `data_transform()` function, which takes an input DataFrame `x` and returns `x` after undergoing the transformation.\n",
    "3. Consider the explanation output types you are interested in, and define the appropriate explanation transforms. In the above example, we considered additive local feature contribution and additive global feature importance explanations. We therefore defined the `transform_explanation_additive_feature_contribution` and `transform_explanation_additive_feature_importance` functions.\n",
    "4. [**Advanced Additional Step**]: The explanation transform functions used above are used to transform the explanation to match the feature space resulting from the data transform function. In some cases, like in Pyreal's built-in one-hot encoder transformer, we may want to *undo* the transformation in the explanation. In these cases, we would define `inverse_transform_explanation_XXX` functions. See the Pyreal documentation for more details about this."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initializing RealApps\n",
    "\n",
    "Now that we have our model transformers, we can gather the other components of the application:\n",
    " - [`model`]: We train our machine learning model using the external system of our choice.\n",
    " - [`feature_descriptions`]: This optional parameter is dictionary of feature names (ie, the data columns) to descriptions, which we can use if our feature names are not naturally readable."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from pyreal import RealApp\n",
    "\n",
    "transformers.append(DataFrameWrapper(StandardScaler()))\n",
    "transformers.append(FeatureSelectTransformer(columns=expected_column_order))\n",
    "\n",
    "feature_descriptions = california_housing.load_feature_descriptions()\n",
    "print(feature_descriptions)\n",
    "\n",
    "realApp = RealApp(model, X_train_orig=X, transformers=transformers, fit_transformers=True, feature_descriptions=feature_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "realApp.predict(X[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyreal import visualize\n",
    "\n",
    "explanation = realApp.produce_local_feature_contributions(X.iloc[1], shap_type=\"kernel\", training_size=150)\n",
    "visualize.plot_top_contributors(explanation[0], select_by=\"absolute\", show=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Pyreal Usability Study",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}